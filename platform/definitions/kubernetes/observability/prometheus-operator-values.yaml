prometheus:
  prometheusSpec:
    # High availability configuration
    replicas: 2
    replicaExternalLabelName: "replica"
    ruleSelectorNilUsesHelmValues: false
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    probeSelectorNilUsesHelmValues: false

    # Retention settings
    retention: 15d
    retentionSize: "50GB"

    # Resource configuration for production
    resources:
      limits:
        cpu: 2000m
        memory: 8Gi
      requests:
        cpu: 1000m
        memory: 4Gi

    # Storage configuration
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: gp2
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 100Gi

    # Security context
    securityContext:
      fsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000

    # Additional configuration for production
    walCompression: true
    enableAdminAPI: false
    externalUrl: "https://prometheus.prism.io"

    # Scrape configuration
    scrapeInterval: "30s"
    evaluationInterval: "30s"

    # Custom scrape configs for non-Kubernetes targets
    additionalScrapeConfigs:
      - job_name: 'blackbox-exporter'
        metrics_path: /probe
        params:
          module: [http_2xx]
        static_configs:
          - targets:
            - https://api.prism.io/health
            - https://app.prism.io
        relabel_configs:
          - source_labels: [__address__]
            target_label: __param_target
          - source_labels: [__param_target]
            target_label: instance
          - target_label: __address__
            replacement: blackbox-exporter:9115

      - job_name: 'drift-detector'
        metrics_path: /metrics
        scheme: https
        static_configs:
          - targets:
            - drift-detector.prism-platform.svc.cluster.local:8080

        # AWS ECS Fargate targets
      - job_name: 'ecs-tasks'
        ec2_sd_configs:
          - region: us-east-1
            profile: default
            port: 9090
        relabel_configs:
          - source_labels: [__meta_ec2_tag_PrometheusEnabled]
            regex: true
            action: keep

alertmanager:
  alertmanagerSpec:
    # High availability configuration
    replicas: 3

    # Resource configuration for production
    resources:
      limits:
        cpu: 200m
        memory: 256Mi
      requests:
        cpu: 100m
        memory: 128Mi

    # Storage configuration
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: gp2
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi

    # Security context
    securityContext:
      fsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000

    # External URL
    externalUrl: "https://alertmanager.prism.io"

    # Configure alertmanager to handle alerts properly
    config:
      global:
        resolve_timeout: 5m
        slack_api_url: '${SLACK_API_URL}'
        pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

      route:
        group_by: ['alertname', 'job', 'service']
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 12h
        receiver: 'slack-notifications'
        routes:
        - match:
            severity: critical
          receiver: 'pagerduty-critical'
          continue: true
        - match:
            severity: warning
          receiver: 'slack-notifications'

      receivers:
      - name: 'slack-notifications'
        slack_configs:
        - channel: '#alerts'
          send_resolved: true
          title: |-
            [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }}
          text: >-
            {{ range .Alerts }}
              *Alert:* {{ .Annotations.summary }}
              *Description:* {{ .Annotations.description }}
              *Severity:* {{ .Labels.severity }}
              *Service:* {{ .Labels.service }}
              *Details:* <{{ .GeneratorURL }}|View in Prometheus>
              {{ if .Annotations.runbook_url }}*Runbook:* <{{ .Annotations.runbook_url }}|Link>{{ end }}
            {{ end }}

      - name: 'pagerduty-critical'
        pagerduty_configs:
        - service_key: '${PAGERDUTY_SERVICE_KEY}'
          send_resolved: true
          description: |-
            [{{ .Status | toUpper }}] {{ .CommonLabels.alertname }} - {{ .CommonAnnotations.summary }}
          details:
            description: '{{ .CommonAnnotations.description }}'
            severity: '{{ .CommonLabels.severity }}'
            service: '{{ .CommonLabels.service }}'
            runbook_url: '{{ .CommonAnnotations.runbook_url }}'

grafana:
  # Admin user configuration
  adminPassword: "${GRAFANA_ADMIN_PASSWORD}"

  # Resource configuration for production
  resources:
    limits:
      cpu: 500m
      memory: 1Gi
    requests:
      cpu: 200m
      memory: 512Mi

  # Persistence configuration
  persistence:
    type: pvc
    enabled: true
    storageClassName: gp2
    accessModes:
      - ReadWriteOnce
    size: 10Gi

  # Security context
  securityContext:
    runAsUser: 472
    runAsGroup: 472
    fsGroup: 472

  # External ingress
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: nginx
      cert-manager.io/cluster-issuer: letsencrypt-prod
    hosts:
      - grafana.prism.io
    tls:
      - secretName: grafana-tls
        hosts:
          - grafana.prism.io

  # Grafana plugins
  plugins:
    - grafana-piechart-panel
    - grafana-worldmap-panel
    - grafana-clock-panel
    - briangann-gauge-panel
    - vonage-status-panel
    - natel-discrete-panel
    - grafana-singlestat-panel

  # Default dashboards
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: 'prism-platform'
          orgId: 1
          folder: 'Prism Platform'
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/prism-platform

  # Datasources configuration
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          url: http://prometheus-server.monitoring.svc.cluster.local:80
          access: proxy
          isDefault: true
        - name: Loki
          type: loki
          url: http://loki-stack.logging.svc.cluster.local:3100
          access: proxy
        - name: Elasticsearch
          type: elasticsearch
          url: http://elasticsearch-client.logging.svc.cluster.local:9200
          access: proxy
          database: "[prism-platform-logs-]YYYY.MM.DD"
          jsonData:
            esVersion: 7
            timeField: "@timestamp"

  # Auto-provisioned dashboards
  dashboards:
    prism-platform:
      api-dashboard:
        json: |
          {
            "title": "API Service Dashboard",
            "uid": "api-service",
            "panels": []
          }
      cell-health-dashboard:
        json: |
          {
            "title": "Cell Health Dashboard",
            "uid": "cell-health",
            "panels": []
          }
      slo-dashboard:
        json: |
          {
            "title": "Service Level Objectives",
            "uid": "slo-dashboard",
            "panels": []
          }

nodeExporter:
  # Resource configuration for production
  resources:
    limits:
      cpu: 200m
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 64Mi

  # Security context
  securityContext:
    runAsNonRoot: true
    runAsUser: 65534
    fsGroup: 65534

kube-state-metrics:
  # Resource configuration for production
  resources:
    limits:
      cpu: 200m
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 128Mi

# Global settings
defaultRules:
  create: true
  rules:
    alertmanager: true
    etcd: true
    general: true
    k8s: true
    kubeApiserver: true
    kubePrometheusNodeAlerting: true
    kubePrometheusNodeRecording: true
    kubernetesAbsent: true
    kubernetesApps: true
    kubernetesResources: true
    kubernetesStorage: true
    kubernetesSystem: true
    node: true
    prometheus: true
    prometheusOperator: true
    time: true

# Security settings
rbac:
  create: true

serviceMonitorSelector:
  matchLabels:
    prometheus: main

# Additional configuration for scraping custom resources
additionalServiceMonitors:
  - name: prism-platform-services
    selector:
      matchLabels:
        monitoring: "enabled"
        app.kubernetes.io/part-of: "prism-platform"
    namespaceSelector:
      any: true
    endpoints:
      - port: metrics
        interval: 15s
        path: /metrics

  - name: prism-platform-cells
    selector:
      matchLabels:
        cell.corrospondent.io/scrape: "true"
    namespaceSelector:
      any: true
    endpoints:
      - port: metrics
        interval: 30s
        path: /metrics

# Implementation instructions:
#
# 1. Update the placeholders for secrets with actual values:
#    - ${SLACK_API_URL}
#    - ${PAGERDUTY_SERVICE_KEY}
#    - ${GRAFANA_ADMIN_PASSWORD}
#
# 2. Install or upgrade Prometheus Operator using this values file:
#    helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
#    helm repo update
#    helm upgrade --install prometheus prometheus-community/kube-prometheus-stack \
#      --namespace monitoring \
#      --create-namespace \
#      -f kubernetes/observability/prometheus-operator-values.yaml
#
# 3. Verify the installation:
#    kubectl get pods -n monitoring
#
# 4. Access Grafana (after setting up proper ingress/DNS):
#    https://grafana.prism.io
#
# 5. For local development/testing:
#    kubectl port-forward svc/prometheus-grafana -n monitoring 3000:80
#    kubectl port-forward svc/prometheus-server -n monitoring 9090:80
#    kubectl port-forward svc/prometheus-alertmanager -n monitoring 9093:9093
