apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-config
  namespace: monitoring
type: Opaque
stringData:
  alertmanager.yaml: |
    global:
      resolve_timeout: 5m
      # Slack configuration
      slack_api_url: 'https://hooks.slack.com/services/REPLACE_WITH_ACTUAL_SLACK_WEBHOOK'
      # PagerDuty configuration
      pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'
      # Email configuration (optional)
      smtp_smarthost: 'smtp.gmail.com:587'
      smtp_from: 'alerts@prism.io'
      smtp_auth_username: 'alerts@prism.io'
      smtp_auth_password: 'REPLACE_WITH_SMTP_PASSWORD'
      smtp_require_tls: true

    # The root route on which each incoming alert enters
    route:
      # Group alerts by their name and service
      group_by: ['alertname', 'service', 'severity']
      # Wait 30s to buffer alerts of the same group before sending
      group_wait: 30s
      # Wait 5m before sending a notification about new alerts for a group
      group_interval: 5m
      # If an alert is not resolved after 12h, resend the notification
      repeat_interval: 12h
      # Default receiver if no matching route
      receiver: 'slack-notifications'

      # Sub-routes for different severity levels
      routes:
      - match:
          severity: 'critical'
        receiver: 'pagerduty-critical'
        # Continue to also send to slack even if PagerDuty is notified
        continue: true
      - match:
          severity: 'warning'
        receiver: 'slack-notifications'
      - match:
          severity: 'info'
        receiver: 'slack-info'

      # Special route for database issues - route to DB team
      - match:
          service: 'database'
        receiver: 'database-team'

      # Special route for security alerts
      - match:
          team: 'security'
        receiver: 'security-team'
        continue: true

    # Receiver definitions
    receivers:
    # Slack receiver for general notifications
    - name: 'slack-notifications'
      slack_configs:
      - channel: '#alerts'
        send_resolved: true
        title: |-
          [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }}
        text: >-
          {{ range .Alerts }}
            *Alert:* {{ .Annotations.summary }}
            *Description:* {{ .Annotations.description }}
            *Severity:* {{ .Labels.severity }}
            *Service:* {{ .Labels.service }}
            {{ if .Labels.instance }}*Instance:* {{ .Labels.instance }}{{ end }}
            *Details:* <{{ .GeneratorURL }}|View in Prometheus>
            {{ if .Annotations.runbook_url }}*Runbook:* <{{ .Annotations.runbook_url }}|Link>{{ end }}
          {{ end }}
        icon_emoji: '{{ if eq .Status "firing" }}:fire:{{ else }}:white_check_mark:{{ end }}'
        footer: 'Prism Platform Monitoring'

    # Slack for info-level notifications
    - name: 'slack-info'
      slack_configs:
      - channel: '#monitoring-info'
        send_resolved: true
        title: |-
          [INFO] {{ .CommonLabels.alertname }}
        text: >-
          {{ range .Alerts }}
            *Info:* {{ .Annotations.summary }}
            *Details:* {{ .Annotations.description }}
            *Service:* {{ .Labels.service }}
          {{ end }}
        icon_emoji: ':information_source:'

    # PagerDuty for critical alerts
    - name: 'pagerduty-critical'
      pagerduty_configs:
      - service_key: 'REPLACE_WITH_PAGERDUTY_SERVICE_KEY'
        send_resolved: true
        description: |-
          [{{ .Status | toUpper }}] {{ .CommonLabels.alertname }} - {{ .CommonAnnotations.summary }}
        client: 'Prism Platform Alertmanager'
        client_url: 'https://prometheus.prism.io'
        details:
          description: '{{ .CommonAnnotations.description }}'
          severity: '{{ .CommonLabels.severity }}'
          service: '{{ .CommonLabels.service }}'
          runbook_url: '{{ .CommonAnnotations.runbook_url }}'
          alert_url: '{{ .CommonAnnotations.dashboard_url }}'

    # Database team notifications
    - name: 'database-team'
      slack_configs:
      - channel: '#db-team'
        send_resolved: true
        title: |-
          [{{ .Status | toUpper }}] Database Alert: {{ .CommonLabels.alertname }}
        text: >-
          {{ range .Alerts }}
            *Alert:* {{ .Annotations.summary }}
            *Description:* {{ .Annotations.description }}
            *Database:* {{ .Labels.database }}
            *Severity:* {{ .Labels.severity }}
            *Details:* <{{ .GeneratorURL }}|View in Prometheus>
            {{ if .Annotations.runbook_url }}*Runbook:* <{{ .Annotations.runbook_url }}|Link>{{ end }}
          {{ end }}
        icon_emoji: ':database:'
      email_configs:
      - to: 'db-team@prism.io'
        send_resolved: true
        html: |
          <!DOCTYPE html>
          <html>
          <body>
            <h2>{{ .Status | toUpper }} Database Alert: {{ .CommonLabels.alertname }}</h2>
            {{ range .Alerts }}
            <p><strong>Alert:</strong> {{ .Annotations.summary }}</p>
            <p><strong>Description:</strong> {{ .Annotations.description }}</p>
            <p><strong>Database:</strong> {{ .Labels.database }}</p>
            <p><strong>Severity:</strong> {{ .Labels.severity }}</p>
            {{ if .Annotations.runbook_url }}
            <p><strong>Runbook:</strong> <a href="{{ .Annotations.runbook_url }}">Link</a></p>
            {{ end }}
            {{ end }}
          </body>
          </html>

    # Security team notifications for security-related alerts
    - name: 'security-team'
      slack_configs:
      - channel: '#security-alerts'
        send_resolved: true
        title: |-
          [{{ .Status | toUpper }}] Security Alert: {{ .CommonLabels.alertname }}
        text: >-
          {{ range .Alerts }}
            *Alert:* {{ .Annotations.summary }}
            *Description:* {{ .Annotations.description }}
            *Severity:* {{ .Labels.severity }}
            *Service:* {{ .Labels.service }}
            {{ if .Labels.instance }}*Instance:* {{ .Labels.instance }}{{ end }}
            *Details:* <{{ .GeneratorURL }}|View in Prometheus>
            {{ if .Annotations.runbook_url }}*Runbook:* <{{ .Annotations.runbook_url }}|Link>{{ end }}
          {{ end }}
        icon_emoji: ':lock:'
      pagerduty_configs:
      - service_key: 'REPLACE_WITH_SECURITY_PAGERDUTY_KEY'
        send_resolved: true
        description: |-
          [{{ .Status | toUpper }}] Security Alert: {{ .CommonLabels.alertname }}
        severity: >-
          {{ if eq .CommonLabels.severity "critical" }}critical{{ else if eq .CommonLabels.severity "warning" }}warning{{ else }}info{{ end }}

    # Special inhibition rules to prevent alert spam
    inhibit_rules:
    # Inhibit all warning alerts if the same alertname/service is critical
    - source_match:
        severity: 'critical'
      target_match:
        severity: 'warning'
      equal: ['alertname', 'service', 'cluster']

    # If a node is down, inhibit all alerts for that node
    - source_match:
        alertname: 'NodeNotReady'
      target_match_re:
        alertname: '.*'
      equal: ['instance']

---
# Deployment instructions:
#
# 1. Create a namespace for monitoring if it doesn't already exist:
#    kubectl create namespace monitoring
#
# 2. Replace placeholder values in this file:
#    - REPLACE_WITH_ACTUAL_SLACK_WEBHOOK
#    - REPLACE_WITH_SMTP_PASSWORD
#    - REPLACE_WITH_PAGERDUTY_SERVICE_KEY
#    - REPLACE_WITH_SECURITY_PAGERDUTY_KEY
#
# 3. Apply the AlertManager configuration:
#    kubectl apply -f kubernetes/observability/alertmanager-config.yaml
#
# 4. If using Prometheus Operator, update the Alertmanager CRD to use this secret:
#    kubectl patch alertmanager alertmanager -n monitoring --type=merge \
#      --patch '{"spec":{"configSecret":"alertmanager-config"}}'
#
# 5. Verify that Alertmanager loaded the configuration successfully:
#    kubectl logs -n monitoring -l app=alertmanager
#
# 6. Test alerts by temporarily lowering threshold values in the Prometheus rules
#    or by using the Alertmanager API to create test alerts.
